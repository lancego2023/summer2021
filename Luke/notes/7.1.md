## Tasks Completed

- 

## Notes

- Researching how to use the Docker SDK in Python to launch multiple processes within the container. I will need to start nvprof then the app (the entrypoint of the image) so that nvprof can attach to it.
- Let's assume that this Python script will be running within a container, let's say the first container. In this case, I hope that the docker python module will be able to connect to the docker socket on its own, despite the different environment. Although I think the Docker socket is the only necessary resource to have to interact with Docker, so it should be fine.
  - The `docker_client.containers.run()` method takes in the arguments of the command line as keyword pairs. It blocks while the container runs. Here is what I currently have for the arguments: `c = docker_cl.containers.run('waggle/plugin-objectcounter:0.0.0', detach=True, runtime='nvidia', volumes={'/home/nvidia/luke_projects/config_dir/data-config.json': {'bind': '/run/waggle/data-config.json', 'mode':'ro'}}, name='objcounter', network='host')`
- **Demo Notes**
  - My goal at the moment is to use the Docker API to launch the app's container, start nvprof as a process within that container, and then start the app's process itself. To do this I am using a Python module which simplifies interactions with the Docker Daemon.
    - I am assuming that the Python script to "run" the pipeline will be within a Docker container spawned by Jenkins? (I'm not certain about that though)
- **Notes**
  - The jenkins container will be running the app container within it
  - Each call to my script will be with different parameters for each run configuration
  - Chris's code will be on the level of the second container and will communicate with the app on a socket level
  - Record tail latency (worst observable latency) and memory (maybe worst case recording?)
  - Own jenkins server that initiates pipeline and grabs output from agent. Data analysis (peaks). Historical performance.

